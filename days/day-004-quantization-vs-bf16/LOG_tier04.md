# üî• **AWQ vs GPTQ ‚Äî Deep, Practical Explanation**

Quantization in LLM inference = **reduce weight precision (usually from FP16/BF16 ‚Üí INT4)** without hurting quality too much, so you:

* fit larger models in VRAM
* serve more concurrent users
* increase throughput

Both **AWQ** and **GPTQ** are *weight-only post-training quantization* methods.
They do **not** quantize activations ‚Äî only weights.

But their philosophy, math, and behavior differ significantly.

---

# üü© **1. AWQ ‚Äî Activation-Aware Weight Quantization (2023)**

### **Core idea:**

**Don't quantize weights blindly.
Quantize them based on how important they are for activation quality.**

AWQ looks at how each weight contributes to activations during inference.
It identifies **‚Äúcritical‚Äù channels / heads / blocks** and selectively *attenuates* quantization error.

### **How AWQ works (mechanics):**

1. **Feed calibration samples (200‚Äì500 tokens typical)** through the FP16 model.
2. Measure **activation sensitivity** for each block/layer.
3. Compute scaling factors:

   * Big weights ‚Üí keep more precision
   * Small/noisy weights ‚Üí can compress safely
4. Apply per-channel INT4 quantization.
5. Produce a quantized checkpoint + scale metadata.

### **Strengths**

| AWQ Strength                 | Why it matters                               |
| ---------------------------- | -------------------------------------------- |
| ‚úî **Excellent stability**    | Less likely to degrade reasoning / coherence |
| ‚úî **Very low VRAM usage**    | Often 40‚Äì60% reduction                       |
| ‚úî **Fast loading & serving** | Simple structure, good for vLLM              |
| ‚úî **Good on long context**   | Important for Qwen2.5 models                 |
| ‚úî **Production-friendly**    | Deterministic, robust                        |

### **Weaknesses**

* Slightly slower TTFT compared to pure FP16 (because of scaling operations).
* Sometimes slightly lower throughput vs GPTQ at batch 1 (rare).

---

# üü• **2. GPTQ ‚Äî Gradient Post-Training Quantization (2022)**

### **Core idea:**

**Quantize weights by solving a one-step optimization problem that minimizes output error.**

GPTQ is more ‚Äúmathematical‚Äù and uses:

* blockwise reconstruction
* error minimization
* quantization-aware second-order approximations (Hessian-based)

### **How GPTQ works (mechanics):**

1. Run calibration samples through FP16 model.
2. Compute approximate Hessian of the weight blocks.
3. Quantize each block while minimizing:

   ```
   || W_fp16  ‚Äì  W_int4 * scale ||   under Hessian weighting
   ```
4. Bake quantized weights into a single safetensors file.

### **Strengths**

| GPTQ Strength                               | Why it matters                |
| ------------------------------------------- | ----------------------------- |
| ‚úî **Slightly faster throughput at batch=1** | Good for low-concurrency apps |
| ‚úî **Often smaller files**                   | More aggressive compression   |
| ‚úî **Works very well on many 7B‚Äì13B models** | Very widely adopted           |

### **Weaknesses**

* **Less stable** than AWQ on reasoning-heavy workloads
* Can produce **more quality artifacts**
* Some GPTQ models suffer from:

  * repetition loops
  * missing token collapse
  * broken long-context behavior
* More variance depending on quantization config (group size, act order, dampening)

---

# ‚ö° Summary Table ‚Äî AWQ vs GPTQ (Engineer Edition)

| Dimension              | **AWQ**                         | **GPTQ**                                      |
| ---------------------- | ------------------------------- | --------------------------------------------- |
| Method Type            | Activation-aware                | Error-minimization                            |
| Quality Stability      | ‚≠ê‚≠ê‚≠ê‚≠ê                            | ‚≠ê‚≠ê‚≠ê                                           |
| VRAM Reduction         | ‚≠ê‚≠ê‚≠ê‚≠ê                            | ‚≠ê‚≠ê‚≠ê‚≠ê                                          |
| Throughput (batch=1)   | ‚≠ê‚≠ê‚≠ê                             | ‚≠ê‚≠ê‚≠ê‚≠ê                                          |
| Throughput (batch>1)   | ‚≠ê‚≠ê‚≠ê‚≠ê                            | ‚≠ê‚≠ê‚≠ê                                           |
| Tail Latency           | Low                             | Medium                                        |
| Long Context Stability | High                            | Medium‚ÄìLow                                    |
| Failure Modes          | Mild degradation                | Repetition, collapse, weird loops             |
| vLLM Compatibility     | Excellent                       | Good but config-dependent                     |
| Best For               | Chatbots, reasoning, production | Lightweight apps, small models, offline tools |

---

# üöÄ In Your Case: **Qwen2.5-1.5B on RTX2000 Ada**

For your exact setup:

* small model (1.5B)
* BF16 baseline already running
* RTX 2000 Ada 16GB
* vLLM as runtime
* you care about **quality**, stability, KV behavior

### ‚≠ê **Recommended: AWQ**

Why?

* Extremely stable for long-context models (Qwen is 32k by default)
* Better behavior under concurrency (vLLM continuous batching)
* More robust to scaling issues when GPU is small (16GB)
* Lower VRAM footprint ‚Üí more KV cache ‚Üí more concurrency
* Lower risk of weird output artifacts during your quality tests (Tier04 Task 1.3)

### GPTQ is still usable, but:

* slightly riskier for reasoning tasks
* may degrade at 4-bit
* more variance depending on quant parameters
* some GPTQ quantizations for Qwen2.5 have been reported to be unstable in vLLM

---

# üìå Recommended Model for Day 004 (again, reinforced)

### ‚úî **`bartowski/Qwen2.5-1.5B-Instruct-AWQ`**

This is the best quantized version of your Day 2 baseline model.

Works flawlessly with:

```bash
--quantization awq
```

---

## Advanced Quantization Topics ‚Äì Concrete Examples

### Quant capacity

**Goal**: Turn ‚ÄúINT4 saves VRAM‚Äù into **hard numbers about extra capacity** on RTX 2000.

- Run a reduced chat grid (e.g. `conc=1,8,16`, `max_tokens=128`) for BF16 and AWQ.  
- Store a merged CSV: `~/benchmarks/day004_quant_capacity_rtx16gb.csv` with columns like `precision, conc, tok_s, p95_ms`.  
- Derive:
  - ‚ÄúMax safe concurrency‚Äù for BF16 vs AWQ at your target p95.  
  - A one-liner, e.g. *‚ÄúOn RTX 2000, AWQ sustains ~1.7√ó more users at p95 ‚â§ 3s.‚Äù*  
- Add a short capacity summary to `day004_quant_vs_bf16_notes.md` under ‚ÄúQuant Capacity on RTX2000‚Äù.

### Quant compute graphs

**Goal**: See **where** quantization is buying you speed (or not) in the kernel timeline.

- Capture a short Nsight Systems trace for a single 200-token generation in BF16 vs AWQ.  
- For each run, screenshot the kernel timeline and annotate:
  - Which kernels shrink / disappear under quant (e.g. GEMMs, dequant ops).  
  - Whether the decode loop still looks memory-bound (lots of small kernels with gaps).  
- Save annotated images under `~/artifacts/day004_quant_compute_graphs/`.  
- In your notes, write 3‚Äì5 bullets answering: *‚ÄúDid INT4 move me closer to a FLOP ceiling or just reduce bandwidth pressure?‚Äù*

### Quant quality failure modes

**Goal**: Build a **small catalog of real failure modes** instead of vague ‚Äúquality may drop‚Äù.

- Construct a 15‚Äì20 prompt set mixing: factual QA, ‚Äúexplain like I‚Äôm 5‚Äù, multi-step reasoning, code, and summarization.  
- Run BF16 and AWQ and log obvious issues into `~/artifacts/day004_quant_quality_failures.md` with a table:
  - `prompt_id`, `category`, `bf16_ok?`, `quant_issue?`, `symptom`, `notes`.  
- Look for patterns:
  - Does AWQ fail more on math? On code? On multi-hop reasoning?  
- Add a ‚ÄúQuant Failure Modes‚Äù subsection to your Day 4 notes with 3 concrete examples you‚Äôd actually show a client.

### Quant cost models

**Goal**: Turn throughput numbers into **$/1M tokens** that product teams understand.

- Pick 1‚Äì2 representative configs (e.g. chat conc=16, batch conc=32).  
- Using your measured tokens/sec and an hourly RTX 2000 price, compute:
  - `cost_per_1M_tokens_bf16` and `cost_per_1M_tokens_awq`.  
- Write a short `~/artifacts/day004_quant_cost_model.md` with:
  - A tiny table summarizing the numbers.  
  - 2 bullets on what this implies for *daily* or *monthly* spend at your expected traffic.  
- Pull a one-sentence takeaway into the case study: *‚ÄúINT4 AWQ cuts serving cost/1M tokens by ~X% at equal p95 latency.‚Äù*

### Quant-under-concurrency

**Goal**: Understand **how quant changes your latency curve**, not just peak throughput.

- Fix `max_tokens` and sweep concurrency upward for BF16 and AWQ until p95 crosses your chat SLO (e.g. 3s).  
- Plot or at least tabulate `conc` vs `p95` for both precisions; store raw data in `~/benchmarks/day004_quant_concurrency_sweep.csv`.  
- Extract:
  - ‚ÄúKnee points‚Äù where p95 starts to blow up.  
  - A rule of thumb, e.g. *‚ÄúOn RTX 2000, AWQ keeps p95 < 3s up to conc‚âà24; BF16 only to conc‚âà14.‚Äù*  
- Add these knee points to your ‚ÄúQuantization Risk Brief‚Äù as concrete operating guidance.

### The real reasons enterprises choose INT4

**Goal**: Distill **business, ops, and platform** reasons‚Äînot just ‚Äúit‚Äôs faster‚Äù.

- Based on your experiments, draft `~/reports/day004_int4_business_case.md` with 5‚Äì8 bullets such as:
  - Higher tenant density per GPU (more workspaces / orgs per card).  
  - Meeting latency SLOs on cheaper SKUs (RTX/A-series instead of A100/H100).  
  - Making A/B experiments cheaper by reducing the GPU footprint for each variant.  
  - Keeping a single ‚ÄúINT4-optimized‚Äù platform config instead of bespoke BF16 setups per model.  
- Close with 2 bullets on **when INT4 is a bad idea** (e.g. safety-critical QA, strict factual accuracy), tied back to the failure modes you observed.

